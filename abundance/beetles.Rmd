---
title: "Beetles"
output: 
  github_document:
    df_print: tibble
---

```{r setup,message=FALSE}
library(lubridate)
library(tidyverse)
```


```{r}
# neonstore::neon_download("DP1.10022.001")
bet_sorting <- neonstore::neon_read("bet_sorting")
# bet_sorting <- read_csv("../data/bet_sorting-basic.csv") # Ben's hack
sites <- neonstore::neon_sites() 


```


```{r}
bet_ts <- bet_sorting %>% 
  select(collectDate, siteID, domainID, scientificName, individualCount) %>%
  mutate(month = format(collectDate, "%Y-%m")) %>% 
  mutate(month = as.Date(paste(month, "01", sep="-"))) %>%
  group_by(scientificName, month, siteID, domainID) %>%
  summarize(count = sum(individualCount, na.rm = TRUE)) %>%
  ungroup()

bet_samples <- bet_sorting %>% 
  select(collectDate, siteID, plotID, domainID, trapID, scientificName, individualCount )%>%
  mutate(month = format(collectDate, "%Y-%m")) %>% 
  mutate(month = as.Date(paste(month, "01", sep="-"))) %>%
  group_by(scientificName, month, siteID, domainID)

```

## Counts

First let's take a quick look at the raw counts data.  This is less meaningful than abundance,
since we would need to account for detection probability, and effort is not entirely constant over
time or necessarily equal across sites.  Details of the estimation of beetle density will need to
take into account the specifics of the pitfall sampling design.  For now, let's consider only the
raw counts, which are much simpler to work with and free from assumptions required to estimate abundance:


To start, here is cumulative counts across all beetles: shows an increase which is no doubt connected 
to increased sampling effort as sites come online, along with an obvious seasonal 
pattern...


```{r}
totals <- bet_ts %>% 
  group_by(month) %>%
  summarize(count = sum(count, na.rm = TRUE)) %>%
  ungroup()

totals %>% ggplot(aes(month, count)) + geom_line() + geom_point()
```


```{r}
bysite <- bet_ts %>% 
  group_by(month, siteID, domainID) %>%
  summarize(count = sum(count, na.rm = TRUE)) %>%
  ungroup()

domains <- sites %>% select(domainCode, domainName) %>% distinct()
bet_domain <- bysite %>% left_join(domains, by = c(domainID = "domainCode"))
```

```{r fig.width=10, fig.height=20}

bet_domain %>% ggplot(aes(month, count)) + geom_line() + geom_point() + 
  facet_wrap(~domainName, ncol = 3, scale = "free_y")
```



-------------------------------------


```{r}
## now the fun starts
library(nimbleEcology)
```

## Estimating abundance

> Ground beetles are sampled using pitfall traps (16 oz deli containers filled with 150 or 250 mL of propylene glycol). Four traps are deployed in each of 10 plots at each terrestrial NEON site (40 traps per site), with traps arrayed approximately 20 meters from the center of the plot in each of the four cardinal directions. Sampling occurs biweekly throughout the growing season (when temperatures are above 4 degrees C).


--------------


## Ben's try


```{r}
library(nimbleEcology)
library(neonstore)
bet_sorting <- neon_read("bet_sorting", dir = "/home/shared-data/neonstore/")
bet_raw <- neon_read("bet_fielddata-basic", dir = "/home/shared-data/neonstore/")
```


Now when I get data for a specific date, I'll associate it with the list stored
in `all_locs` and put in 0s for any site that didn't have species collected
on that date.

I'm going to start by taking a look at just one site & one species.


```{r}
prep_dat_onesitedate <- function(site, date, species,
                                 bet_sorting, bet_raw) {
  spec_site_dat <- bet_sorting %>% 
    filter(scientificName == species, 
           siteID == site,
           collectDate == date) %>% 
    select(siteID, plotID, trapID, collectDate, individualCount) %>% 
    arrange(plotID, trapID) %>% 
    distinct()
  
  all_locs_this_sitedate <- bet_raw %>% 
    filter(siteID == site) %>% 
    filter(collectDate == date) %>% 
    select(siteID, plotID, trapID, collectDate) %>% 
    distinct()
    
  all_loc_data <- left_join(all_locs_this_sitedate, spec_site_dat,
                            by = c("siteID", "plotID", "trapID"))
  all_loc_data$collectDate <- date
  all_loc_data$individualCount[is.na(all_loc_data$individualCount)] <- 0
  
  all_loc_data <- all_loc_data %>% 
    mutate(plot_int = as.numeric(as.factor(paste0(siteID, plotID, collectDate)))) %>% 
    arrange(plot_int)    
  
  return(all_loc_data)
}

# The following code gets me the start and end vectors of the data each plot
get_start_end <- function(model_data) {
  start_vec <- numeric(max(model_data$plot_int))
  end_vec <- numeric(max(model_data$plot_int))
  
  for (i in 1:length(start_vec)) {
    start_vec[i] <- min(which(model_data$plot_int == i))
    end_vec[i] <- max(which(model_data$plot_int == i))
  }
  return(list(start = start_vec, end = end_vec))
}


```

Apply the functions I wrote to a selected site and date.

```{r}
model_data <- prep_dat_onesitedate(site = "ORNL", date = as.Date("2014-06-03"),
                                   species = "Cyclotrachelus fucatus",
                                   bet_sorting, bet_raw)
start_end_vecs <- get_start_end(model_data)
```


Now I put together a nimbleModel for these data.

```{r}
nmix_onesite_code <- nimbleCode({
  for (plot in 1:nplot) {
    # Each set 
    y[start[plot]:end[plot]] ~ 
        dNmixture_s(lambda = lambda,
                    prob = prob,
                    Nmin = 0,
                    Nmax = 800,
                    len = 1 + end[plot] - start [plot])
  }
  
  lambda ~ dunif(0, 10000)
  prob ~ dunif(0, 1)
})

onesite_model <- nimbleModel(nmix_onesite_code,
                             constants = list(start = start_end_vecs$start,
                                              end = start_end_vecs$end,
                                              nplot = max(model_data$plot_int)),
                             data = list(y = model_data$individualCount),
                             inits = list(prob = 0.5, 
                                          lambda = mean(model_data$individualCount) * 2))
```

With this very simple model, we could use MCMC or MLE. I'll demo an easy MCMC
workflow so that we can use it even as the model gets more complicated.


```{r}
onesite_MCMC <- buildMCMC(onesite_model)

Cmodel <- compileNimble(onesite_model)
CMCMC <- compileNimble(onesite_MCMC)

```

```{r}
samples <- runMCMC(CMCMC, niter = 50000, nburnin = 5000)

plot(samples[,1], type = "l")
plot(density(samples[,1]))

plot(samples[,2], type = "l")
plot(density(samples[,2]))
```

This mixing looks decent now.

### Ben's try #2: multiple dates

Now I'll look at multiple sampling periods. I'll put month as a covariate on
abundance and plot as a random effect on detection probability.

```{r}
# Grab the data. Apply the original function across any dates indicated for it.
dates_needed <- bet_sorting %>% 
                filter(siteID == "ORNL") %>% 
                select(collectDate) %>%
                unique()

# Grab all the data for each date, then combine into one dataframe.
model_data_list <- lapply(dates_needed$collectDate,
                          prep_dat_onesitedate,
                          site = "ORNL", species = "Cyclotrachelus fucatus",
                          bet_sorting = bet_sorting,
                          bet_raw = bet_raw)
model_data_ts <- bind_rows(model_data_list)
# Redo the plot_ints
model_data_ts <- model_data_ts %>% 
  mutate(plot_int = as.numeric(as.factor(paste0(siteID,
                                                plotID,
                                                collectDate)))) %>% 
  arrange(plot_int)

start_end_vecs_ts <- get_start_end(model_data_ts)
```




```{r}
nmix_spacetime_code <- nimbleCode({
  
  # for each spacetime plot...
  for (st in 1:nspacetime) {
    
    # Mean abundance is a log-linked linear combo with which month it is
    log(lambda[st]) <- abund_int + beta_month * month[st] + 
                        beta_month_sq * month[st] * month[st]
    
    # Prob of detection is a logit-linked random effect on which geog plot it is
    logit(prob[st]) <- prob_int + pranef[geoplot[st]]
    
    # Use latent states
    N[st] ~ dpois(lambda[st])
    for (i in start[st]:end[st]) y[i] ~ dbinom(N[st], prob = prob[st])
  }
  
  
  # Priors
  beta_month ~ dnorm(0, sd = 1000)
  beta_month_sq ~ dnorm(0, sd = 1000)
  abund_int ~ dnorm(0, sd = 1000)
  prob_int ~ dnorm(0, sd = 1000)
  pranef_sd ~ dunif(0, 100)
  for (i in 1:ngeoplots) {
    pranef[i] ~ dnorm(0, sd = pranef_sd)
  }
})

spacetime_model <- nimbleModel(nmix_spacetime_code,
                             constants = list(start = start_end_vecs_ts$start,
                                              end = start_end_vecs_ts$end,
                                              nspacetime = length(start_end_vecs_ts$end),
                                              ngeoplots = length(unique(model_data_ts$plotID)),
                                              month = as.numeric(scale(month(model_data_ts$collectDate))),
                                              geoplot = as.numeric(as.factor(model_data_ts$plotID))
                                              ),
                             data = list(y = model_data_ts$individualCount),
                             inits = list(prob_int = 0, 
                                          pranef_sd = 1,
                                          pranef = rnorm(length(unique(model_data_ts$plotID)), 0, 1),
                                          abund_int = log(mean(model_data_ts$individualCount) * 2),
                                          beta_month = 0,
                                          beta_month_sq = 0))
```


Back to MCMC.


```{r}
spacetime_MCMC <- buildMCMC(spacetime_model)

Cmodel <- compileNimble(spacetime_model)
CMCMC <- compileNimble(spacetime_MCMC)
```


```{r}
samples <- runMCMC(CMCMC, niter = 50000, nburnin = 1000)
for (i in 1:ncol(samples)) {
  plot(samples[,i], type = "l", main = colnames(samples)[i])
  plot(density(samples[,i]), main = colnames(samples)[i])
}
``` 

This is still mixing poorly but probably isn't worth worrying too much about at this step.

Our predicted values of interest:

```{r}
# Mean expectation of abundance
exp(mean(samples[,"abund_int"]))
# Quadratic relationship btw month abundance on the log scale (probably not a great model)
mean(samples[,"beta_month"])
mean(samples[,"beta_month_sq"])
# mean expectation of detection rate
expit(mean(samples[, "prob_int"]))
# How much does detection vary from plot to plot, on the logit scale?
mean(samples[, "pranef_sd"])
```

### Ben's try #3: use MLE

We can't quite use the above model for MLE because of the many random effect
latent states in p_ranef. If we change the way we parameterize the model
to include only fixed effects, we can do MLE.

(As a side note, MLE on the above model should become possible soon when NIMBLE's
Laplace approximation method is out.)

Until then, let's say the covariate on detection is a fixed effect on year, why not.

```{r}
nmix_spacetime_code <- nimbleCode({
  
  # for each spacetime plot...
  for (st in 1:nspacetime) {
    
    # Mean abundance is a log-linked linear combo with which month it is
    log(lambda[st]) <- abund_int + beta_month * month[start[st]] + 
                        beta_month_sq * month[start[st]] * month[start[st]]
    
    # Prob of detection is a logit-linked random effect on which geog plot it is
    logit(prob[st]) <- p_eff[year[start[st]]]
    
    # Each set 
    y[start[st]:end[st]] ~ 
        dNmixture_s(lambda = lambda[st],
                    prob = prob[st],
                    Nmin = 0,
                    Nmax = 1000,
                    len = 1 + end[st] - start[st])
  }
  
  
  # For MLE, we DON'T use priors, since we don't want these likelihoods
  #   to end up in the calculation of log prob of y
  # beta_month ~ dnorm(0, sd = 1000)
  # beta_month_sq ~ dnorm(0, sd = 1000)
  # abund_int ~ dnorm(0, sd = 1000)
  # prob_int ~ dnorm(0, sd = 1000)
  # for (i in 1:nyear) {
  #   p_eff[i] ~ dnorm(0, sd = 1000)
  # }
})

spacetime_model <- nimbleModel(nmix_spacetime_code,
   constants = list(start = start_end_vecs_ts$start,
                    end = start_end_vecs_ts$end,
                    nspacetime = length(start_end_vecs_ts$end),
                    nyear = length(unique(year(model_data_ts$collectDate))),
                    month = as.numeric(scale(month(model_data_ts$collectDate))),
                    year = as.numeric(as.factor(year(model_data_ts$collectDate)))
                    ),
   data = list(y = model_data_ts$individualCount),
   inits = list(p_eff = rep(0, length(unique(year(model_data_ts$collectDate)))),
                abund_int = log(mean(model_data_ts$individualCount) * 2),
                beta_month = 0,
                beta_month_sq = 0), 
   calculate = F)

# This is the log probability of the whole model (in this case Pr(y|theta))
spacetime_model$calculate()
```

Here's a basic MLE pipeline.
Basically, we're going to use R's optim function and pass it a compiled nimbleFunction
that updates the values of the model we're interested in and returns logprob.

```{r}
wrt_nodes <- spacetime_model$expandNodeNames(c("p_eff", "abund_int", 
                                               "beta_month", "beta_month_sq"))
wrt_inits <- values(spacetime_model, wrt_nodes)


# setAndCalculate returns a compilable object (nimbleFunction) that sets
# nodes for a target value and then calculates the log likelihood
my_sc <- setAndCalculate(model = spacetime_model, targetNodes = wrt_nodes)

Cspacetime_model <- compileNimble(spacetime_model)
Csc <- compileNimble(my_sc)

Csc$run(rep(0.2, 10)) # Try once with some arbitrary parameter values
Cspacetime_model$abund_int # Note that it actually modified the model globally
```


Now a call to optim with the objective function `Csc$run`. Fo

As a note, NIMBLE has a feature in development which will handle model optimization
a lot better using some internal tracking of derivatives that I won't claim to
understand. But, soon, we may be able to replace this with `nimOptim()`.

```{r}
# This takes a while to run with all the data we have.
# It will get slower if we add more data or more parameters (dimensions to optim)
optim_result <- optim(par = wrt_inits,
                      fn = Csc$run,
                      hessian = TRUE,
                      control = list(fnscale= -1))  # This control arg says we want max, not min

param_ests <- tibble(param = wrt_nodes)

# The parameter estimates are the optima:
param_ests$mean <- optim_result$par

# The estimated variance is the absolute value of the diagonal of the inverse of 
# the (negative, bc we inverted it during optim) Hessian at the max
param_ests$var <- abs(diag(solve(-optim_result$hessian)))
param_ests$sd <- sqrt(param_ests$var)

param_ests
```





